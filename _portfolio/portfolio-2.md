---
title: "Enhancing HRI Communication: Proactive Accent Recognition and Language Switching Robot"
excerpt: "The central problem regarding verbal communication in Human-Robot Interaction (HRI) is an often lack of verbal understanding between the human and robot involved in the interaction, and this phenomena can further manifest itself if a non-native English speaker is attempting to interact with an English-speaking robot. Developing a solution towards this problem is important from the perspective of enhanced interactions between humans and robots and overall ease-of-use. 
The central research question for this project is “Does a robot that actively recognizes the native language of a human interacter based on their accent and prompts the need to communicate in that language (Proactive robot), imprint a better impression on the human interacter than a robot that switches language only when prompted (Reactive robot)?” The two hypotheses developed alongside the research question are “a robot that actively changes language based on detected accent leaves the human interacter with a better impression than a robot that changes language only when prompted” and “A robot that fails to correctly recognize the native language of a human interacter does not yield a worse impression than a robot that changes language only when prompted”.  <br/><img src='/images/Nao.png'>"
collection: portfolio
---
Our approach to implement the prototype will consist of 4 components: 1.) the machine learning model for accent recognition 2.) the NAO robot with Choregraphe blocks implemented 3.) the python code for recording voices on laptop 4.) the python code for sending the model result to Nao robot. To be more specific, for the accent recognizer model, we fine tuned on a accent classification model provided by the Hugging face API. The model is implemented based on Emphasized Channel Attention, Propagation and Aggregation in Time Delay Neural Network (ECAPA-TDNN) and Wav2Vec 2.0/XLSR (a Cross-lingual Representation Learning model build based on a Framework for Self-Supervised Learning of Speech Representations) architectures. The model provided the ability to read in and recognize 16 different English accents. For the NAO robot, the interactive system is implemented using Choregraphe blocks including Python code block (for retrieving messages sent from laptop, discussed in detail later), Speech recognition, Set Languages (for both recognition and speaking) and timeframe.
For the python code to record voice, we implemented this functionality because the storage of the Nao robot is limited. We cannot deploy the whole model onto the robot. The solution we came up with is to run our model and record participants' interaction voices on the laptop and send the classification result from our laptop to the robot for further interactions. The python package we used to achieve this recording functionality is sounddevice.
Finally, for the python code for sending results to the robot, as mentioned earlier, we have to send the accent recognizing result back to Nao robot for further interaction. The approach we use is the Secure copy protocol (scp). Using that protocol, we will be able to send (copy) a file with the accent result inside it into the Nao robot’s memory. We can then access the Nao memory to retrieve the result.
<video width="400" controls> <source src="/files/Nao_video.mp4" type="video/mp4">Your browser does not support the video tag.</video>
